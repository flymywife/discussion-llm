{
  "topic": "Discuss the usefulness of Retrieval",
  "rounds": [
    {
      "Model 1 (Vector Search)": {
        "system_prompt": "You are an AI assistant that uses vector search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how vector search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 1 (Vector Search)'s Response:",
        "response": "Retrieval plays a crucial role in enhancing models' knowledge bases and capabilities across various domains such as healthcare, finance, and law. In the context of Language Model Machines (LLMs), the use of retrieval methods like retrieval augmented generation (RAG) has proven to be effective in enabling LLMs to obtain relevant information from knowledge sources and incorporate it into generated text without directly changing the model weights. This approach enhances the model's performance on new tasks by modifying the input query.\n\nThe comparison between fine-tuning and retrieval augmented generation (RAG) in knowledge injection has shown that while fine-tuning is beneficial for several applications, RAG is a more reliable choice for incorporating knowledge into models. The study highlights the importance of continually researching and exploring different combinations of techniques, including diverse auxiliary knowledge bases, to improve the results further.\n\nMoreover, the limitations of models and the impact of hyperparameters on results emphasize the need for thorough optimization and testing in specific cases. The study also emphasizes the importance of considering different knowledge representation perspectives in LLMs and conducting further research to measure knowledge accurately in these models.\n\nOverall, retrieval methods like RAG offer a valuable approach to enhancing models' knowledge bases, improving performance on new tasks, and optimizing the contextual relevance of models in specialized domains. Further research and exploration in this area are essential to advance our understanding of knowledge injection and representation in LLMs."
      },
      "Model 2 (Keyword Search)": {
        "system_prompt": "You are an AI assistant that uses keyword search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how keyword search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 2 (Keyword Search)'s Response:",
        "response": "Keyword search has been incredibly useful in retrieving relevant information from academic papers related to the topic of \"Retrieval.\" By searching for keywords like \"knowledge injection,\" \"fine-tuning,\" and \"retrieval augmented generation,\" I was able to gather insights on how retrieval plays a crucial role in enhancing the knowledge base of Large Language Models (LLMs).\n\nThe search results highlight the importance of retrieval in improving the factual knowledge and capabilities of LLMs in various domains such as healthcare, finance, and law. Retrieval methods like Retrieval Augmented Generation (RAG) use information retrieval techniques to help LLMs obtain and incorporate relevant information from knowledge sources into generated text, thus enhancing their performance on new tasks.\n\nComparing different approaches such as fine-tuning and retrieval augmented generation, the research suggests that while fine-tuning can be useful for many use-cases, retrieval augmented generation (RAG) is considered a more reliable choice for knowledge injection. By utilizing retrieval methods, LLMs can access a wealth of information from external sources, making them more versatile and capable of handling complex tasks.\n\nFurthermore, the search results emphasize the need for further research in the field of knowledge representation in LLMs and the optimization of hyperparameters to improve the effectiveness of retrieval methods. The limitations mentioned in the research, such as the choice of hyperparameters and the source of knowledge bases, underscore the importance of continuous exploration and refinement in utilizing retrieval techniques for enhancing the knowledge and capabilities of LLMs.\n\nIn conclusion, keyword search has allowed me to delve into the significance of retrieval methods in enhancing the knowledge base of LLMs, showcasing the usefulness of retrieval in improving the performance and adaptability of these language models across different domains."
      }
    },
    {
      "Model 1 (Vector Search)": {
        "system_prompt": "You are an AI assistant that uses vector search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how vector search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 1 (Vector Search)'s Response:",
        "response": "Retrieval plays a crucial role in enhancing the knowledge base of models like Large Language Models (LLMs) as discussed in the provided search results. One method of knowledge injection in these models is through retrieval augmented generation (RAG), which uses information retrieval techniques to extract relevant information from a knowledge source and incorporate it into generated text. This approach enables LLMs to leverage external knowledge efficiently, leading to improved performance in various tasks.\n\nVector search, which is based on similarity calculations in a high-dimensional space, has been instrumental in facilitating retrieval tasks in the context of enhancing LLMs with additional knowledge. By mapping text data into vectors, vector search allows efficient matching of queries with relevant information embeddings, enabling quick and accurate retrieval of knowledge crucial for model enrichment.\n\nIn the comparison between fine-tuning and retrieval augmented generation for knowledge injection in LLMs, it was found that RAG was a more reliable choice. The ability of retrieval methods to fetch relevant information from external sources without directly changing the model's weights offers a flexible and effective way to expand the knowledge base of LLMs.\n\nOverall, retrieval, especially through techniques like RAG, demonstrates the usefulness of incorporating external knowledge into models to enhance their understanding, reasoning, and performance across various domains. Vector search plays a pivotal role in supporting such retrieval mechanisms, enabling the efficient access and utilization of external knowledge for model enrichment and improvement."
      },
      "Model 2 (Keyword Search)": {
        "system_prompt": "You are an AI assistant that uses keyword search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how keyword search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 2 (Keyword Search)'s Response:",
        "response": "Keyword search has been a valuable tool in retrieving relevant information from the provided research papers on the topic of knowledge injection in language models (LLMs). In the search results, it is evident that retrieval plays a crucial role in enhancing the factual knowledge and capabilities of LLMs in various domains such as healthcare, finance, and law.\n\nOne key aspect highlighted in the search results is the comparison between fine-tuning and retrieval augmented generation (RAG) approaches for knowledge injection in LLMs. While fine-tuning involves adapting the model using task-specific data, RAG utilizes information retrieval techniques to obtain relevant information from a knowledge source and incorporate it into generated text. The study suggests that RAG is a more reliable choice for knowledge injection, emphasizing the importance of retrieval mechanisms in enhancing a model's knowledge base.\n\nMoreover, the search results discuss the limitations and areas for further research in the field of knowledge representation in LLMs. It points out the significance of exploring different techniques, hyperparameters optimization, and diverse auxiliary knowledge bases to improve the understanding of knowledge injection through retrieval mechanisms.\n\nOverall, the search results demonstrate the usefulness of retrieval in acquiring relevant data and information to enhance the performance and capabilities of language models. By utilizing keyword search to navigate through academic papers, valuable insights regarding the comparison of fine-tuning and retrieval approaches in knowledge injection have been highlighted, showcasing the importance of retrieval mechanisms in advancing the field of natural language processing."
      }
    },
    {
      "Model 1 (Vector Search)": {
        "system_prompt": "You are an AI assistant that uses vector search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how vector search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 1 (Vector Search)'s Response:",
        "response": "Retrieval plays a crucial role in enhancing the performance and capabilities of Language Model Models (LLMs) by enabling them to access and incorporate relevant information. One method discussed in the search results is the use of retrieval augmented generation (RAG), which leverages information retrieval techniques to help LLMs retrieve and integrate knowledge from external sources into generated text. RAG, as highlighted in the research papers, is deemed as a more reliable choice for knowledge injection compared to fine-tuning in certain contexts.\n\nVector search has been instrumental in retrieving information from these academic papers to shed light on the comparison between fine-tuning and retrieval approaches for knowledge injection in LLMs. By analyzing the information retrieved through vector search, we can discern how different techniques like fine-tuning and retrieval augmented generation impact the ability of LLMs to comprehend and manipulate information effectively.\n\nMoreover, the search results mention the importance of exploring diverse auxiliary knowledge bases and optimizing hyperparameters for specific cases to enhance the effectiveness of these approaches. This underscores the practical significance of retrieval not only in accessing relevant information but also in refining the methods used for knowledge injection in LLMs.\n\nIn conclusion, retrieval, particularly through methods like RAG, proves to be valuable for augmenting the knowledge base of LLMs. The ability to retrieve information from varied sources and integrate it seamlessly into text generation processes showcases the usefulness of retrieval techniques in advancing the capabilities of language models in understanding and utilizing information effectively."
      },
      "Model 2 (Keyword Search)": {
        "system_prompt": "You are an AI assistant that uses keyword search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how keyword search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 2 (Keyword Search)'s Response:",
        "response": "Keyword search has been incredibly useful in retrieving relevant information from the provided research papers related to the comparison of fine-tuning and retrieval in enhancing a model's knowledge base. By searching for terms like \"Retrieval,\" \"knowledge injection,\" \"fine-tuning,\" and \"LLMs,\" I was able to glean insights into the different methods used to enhance a model's understanding and ability to retrieve factual data.\n\nFrom the search results, it is evident that retrieval augmented generation (RAG) and in-context learning (ICL) are two methods that leverage information retrieval techniques to enhance Language Model Models (LLMs) by enabling them to retrieve relevant information from knowledge sources and incorporate it into generated text. RAG, a form of ICL, is highlighted as a more reliable choice for knowledge injection compared to fine-tuning in certain contexts.\n\nThe research discussed in the papers emphasizes the importance of optimizing models for specific tasks through fine-tuning and retrieval methods. Furthermore, it acknowledges the need for further research to explore different techniques and knowledge bases to improve the effectiveness of knowledge injection in LLMs.\n\nIn conclusion, the retrieval of information through keyword search has been instrumental in uncovering the usefulness of different methods like fine-tuning and retrieval for enhancing a model's knowledge base, understanding, and data retrieval capabilities. This demonstrates how retrieval methods play a crucial role in advancing the field of artificial intelligence and machine learning."
      }
    },
    {
      "Model 1 (Vector Search)": {
        "system_prompt": "You are an AI assistant that uses vector search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how vector search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 1 (Vector Search)'s Response:",
        "response": "Retrieval plays a crucial role in enhancing the knowledge and capabilities of models in various fields such as healthcare, finance, and law. In the context of Language Model Machines (LLMs), retrieval augmented generation (RAG) has been highlighted as an important method to improve LLMs' performance on new tasks without directly changing their weights. RAG leverages information retrieval techniques to enable LLMs to retrieve relevant information from a knowledge source and incorporate it into generated text.\n\nIn the study comparing fine-tuning and retrieval augmented generation for knowledge injection, it was found that RAG is a more reliable choice. While fine-tuning remains valuable for many use-cases, RAG was identified as a superior method for knowledge injection. Further research is suggested to explore a combination of techniques and different auxiliary knowledge bases to potentially yield enhanced results. It is emphasized that optimizing hyperparameters and evaluating generalization to other LLMs are crucial steps in improving the efficacy of retrieval methods in enhancing models' knowledge base.\n\nThe limitations of machine learning applications, such as the impact of hyperparameters on results and the importance of testing generalization to different models, further underscore the need for continuous research and refinement in retrieval techniques. The study acknowledges that more research is needed to address the theoretical aspects of knowledge representation in LLMs and to explore diverse perspectives on measuring knowledge.\n\nOverall, the search results demonstrate the usefulness of retrieval techniques, particularly RAG, in enhancing LLMs' knowledge base and performance on new tasks. The comparison with fine-tuning showcases the advantages of retrieval methods for knowledge injection, highlighting the importance of ongoing research and optimization to further leverage retrieval in improving the capabilities of models."
      },
      "Model 2 (Keyword Search)": {
        "system_prompt": "You are an AI assistant that uses keyword search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how keyword search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 2 (Keyword Search)'s Response:",
        "response": "Keyword search has played a significant role in retrieving relevant information from academic papers related to the topic of \"Discussing the usefulness of Retrieval.\" The search results highlight the importance of retrieval in the context of enhancing models' knowledge bases, particularly Language Model Models (LLMs).\n\nThe papers discuss various methods to improve models' factual knowledge and capabilities in fields such as healthcare, finance, and law. One of the main focuses is on evaluating a model's ability to memorize, understand, and retrieve factual data efficiently. One approach mentioned is fine-tuning, which involves adapting the model using task-specific data to optimize its performance for specialized domains.\n\nAnother method discussed is in-context learning (ICL), where the model's performance on new tasks is improved by modifying the input query without directly changing the model's weights. One form of ICL mentioned is retrieval augmented generation (RAG), which uses information retrieval techniques to obtain relevant information from a knowledge source and incorporate it into generated text.\n\nThe papers compare the effectiveness of fine-tuning and retrieval augmented generation for knowledge injection. While both approaches have their merits, it is suggested that RAG is a more reliable choice for enhancing models' knowledge bases. The discussion also highlights the need for further research on knowledge representation in LLMs and the importance of optimizing hyperparameters for specific cases.\n\nIn conclusion, the search results demonstrate how keyword search has helped in gathering valuable insights into the usefulness of retrieval methods, such as fine-tuning and retrieval augmented generation, in enhancing models' knowledge bases. The ability to retrieve relevant information from academic papers has provided a comprehensive understanding of the importance of retrieval techniques in improving model performance and capabilities."
      }
    },
    {
      "Model 1 (Vector Search)": {
        "system_prompt": "You are an AI assistant that uses vector search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how vector search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 1 (Vector Search)'s Response:",
        "response": "Retrieval, particularly in the context of information retrieval techniques such as retrieval augmented generation (RAG), plays a significant role in enhancing the knowledge base of pre-trained Language Models (LLMs). The search results indicate that while fine-tuning methods like exposing models to specific knowledge bases can optimize their performance for targeted applications, retrieval augmented generation (RAG) enables LLMs to obtain relevant information from external sources and incorporate it into generated text without directly changing the model weights.\n\nThe comparison between fine-tuning and retrieval augmented generation in the context of knowledge injection highlights the usefulness of retrieval methods. RAG has been found to be a more reliable choice for knowledge injection, indicating its effectiveness in facilitating LLMs to access and incorporate information from external knowledge sources. This approach not only enhances the model's performance on new tasks but also broadens its knowledge base without extensive retraining.\n\nThe search results also suggest that further research is needed to optimize hyperparameters, explore diverse auxiliary knowledge bases, assess knowledge representation in LLMs, and measure knowledge effectively. These findings emphasize the ongoing efforts in improving retrieval techniques to enhance the overall performance and capabilities of LLMs in comprehending and manipulating information from various sources.\n\nIn conclusion, retrieval methods like retrieval augmented generation provide a valuable mechanism for integrating external knowledge sources into pre-trained models. By enabling LLMs to retrieve relevant information and incorporate it into their text generation processes, retrieval approaches contribute to enhancing the models' knowledge base and contextual relevance in specialized domains. Further research and advancements in retrieval techniques are essential to continue improving LLMs' ability to acquire and utilize information effectively."
      },
      "Model 2 (Keyword Search)": {
        "system_prompt": "You are an AI assistant that uses keyword search to retrieve information from academic papers. \n            Your task is to discuss the following topic: \"Discuss the usefulness of Retrieval\"\n            Use the provided search results to inform your discussion. Focus on how keyword search has helped you find relevant information.",
        "user_prompt": "Based on the following search results from research papers, provide your perspective on the topic: 'Discuss the usefulness of Retrieval'.\n\n        Search Results:\n        Note that prehistory involves\nquestions spanning all non-modern history. This approach\naims to enable us to test LLM proficiency in comprehending\nand manipulating information in isolation from its reasoning\nprocesses. Current Events Task To further isolate LLMs’ abili-\n4Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nTable 1. Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).\n\nVarious models have been suggested\nto improve factual knowledge and capabilities in diverse\nfields such as healthcare (Singhal et al., 2023a;b; Wu et al.,\n2023a), finance (Wu et al., 2023b; Yang et al., 2023), and\nlaw (Huang et al., 2023; Nguyen, 2023). In this work, we focus on the evaluation of a model’s knowl-\nedge and its ability to memorize, understand, and retrieve\nfactual data. We aim to understand the concept of knowl-\nedge injection (Wang et al., 2020; Chen et al., 2022; Liu\net al., 2020; Lauscher et al., 2020). Given some knowledge\nbase in the form of a text corpus, what is the best way to\nteach a pre-trained model this knowledge? One way to add knowledge to a pre-trained model is through\nfine-tuning. With fine-tuning, we continue the model’s train-\ning process and adapt it using task-specific data. By expos-\ning the model to a specific knowledge base, we expect the\nmodel weights to adapt accordingly. This process is meant\nto optimize the model for targeted applications, enhanc-\ning its performance and contextual relevance in specialized\ndomains. Another method to enhance a model’s knowledge base is\nthrough the use of in-context learning (ICL) (Chen et al.,\n2021; Radford et al., 2019; Min et al., 2021; Lampinen\net al., 2022). The main idea behind ICL is to improve the\nperformance of pre-trained LLMs on new tasks by modify-\ning the input query to the model without directly changing\nthe weights of the model. One form of ICL is retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Neelakantan\net al., 2022). RAG uses information retrieval techniques to\nenable LLMs to obtain relevant information from a knowl-\nedge source and incorporate it into generated text. This study aims to evaluate the knowledge injection capa-\n1arXiv:2312.05934v3  [cs.AI]  30 Jan 2024Fine-Tuning or Retrieval?\n\nThis is among the first studies to compare two\nprominent approaches in this domain, namely fine-tuning\nand retrieval augmented generation. While fine-tuning can\nbe useful for many use-cases, we found that RAG is a more\nreliable choice for knowledge injection. Some aspects of this work still warrant further research. For\nexample, we focused on unsupervised training as our pri-\nmary fine-tuning method, as opposed to instruction-tuning\nor RL-based methods. Researching combinations of var-\nious techniques, with diverse auxiliary knowledge bases,\nmay yield improved results. This approach, combined with\nour hypothesis from Section 6, could further enhance our\nunderstanding of knowledge injection via FT. While we believe that this work further enhances our under-\nstanding of knowledge in LLMs, there is a lot more work\nto be done in this field. Specifically, more research is re-\nquired regarding the question of knowledge representation\nin LLMs, especially from a theoretical perspective. Finally, further efforts are needed to measure knowledge\nin LLMs. While we employed an empirical approach as\ndescribed in Equation (2), it is important to explore other\ndefinitions and perspectives on knowledge as well, and ex-\ntend upon this work. 8. Limitations\nAs in all machine learning applications, the choice of hyper-\nparameters significantly impacts the results. We therefore\nstrongly recommend optimizing all relevant hyperparame-\nters for specific cases. We have supported our claims by running the experiments\non three different models. However, generalization to other\nLLMs should be tested thoroughly. For example, GPT-4\nachieves near perfect accuracy for some MMLU tasks (Nori\net al., 2023), and thus further improvement is not applicable. Finally, while we chose various topics for the knowledge\nbases, all of our sources came from Wikipedia. Other\ndatasets may yield different results, and must be evaluated\ncarefully. 8Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\nReferences\nAttardi, G. Wikiextractor. https://github.com/\nattardi/wikiextractor , 2015. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O.\n\n        Model 2 (Keyword Search)'s Response:",
        "response": "Retrieval plays a crucial role in knowledge enhancement and utilization within various fields such as healthcare, finance, and law, as highlighted in the provided search results. The ability to retrieve factual data, information, and knowledge is essential for the training and optimization of models, particularly Large Language Models (LLMs), which are designed to comprehend and manipulate information effectively.\n\nOne of the methods discussed in the search results is fine-tuning, where models are trained on specific data to adapt and enhance their performance for targeted applications. Fine-tuning allows for the optimization of models based on the knowledge base provided, leading to improved contextual relevance and outcomes in specialized domains.\n\nAnother significant approach mentioned is retrieval augmented generation (RAG), which utilizes information retrieval techniques to enable LLMs to acquire relevant information from knowledge sources and incorporate it into generated text. This method showcases the importance of retrieval in enhancing a model's knowledge base for better performance on new tasks without directly altering the model's weights.\n\nThe comparison between fine-tuning and retrieval augmented generation in the search results highlights the effectiveness of retrieval in knowledge injection. While fine-tuning has its benefits, RAG is considered a more reliable choice for improving models' knowledge and capabilities.\n\nFurthermore, the limitations and areas for further research mentioned in the search results emphasize the need for continuous exploration and improvement in understanding knowledge representation in LLMs. Optimization of hyperparameters, testing on different models, exploring diverse knowledge bases beyond Wikipedia, and evaluating various techniques are crucial for advancing the field of knowledge injection in models.\n\nIn conclusion, retrieval, whether through fine-tuning or techniques like RAG, proves to be an invaluable tool in enhancing models' knowledge and performance across different domains. Further research and optimization efforts are necessary to unlock the full potential of retrieval in enriching the capabilities of Large Language Models."
      }
    }
  ]
}